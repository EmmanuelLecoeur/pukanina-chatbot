{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Vectoriseurs_themes.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8qV7VCt4KJeHno6B+GICQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iaLUpDXwkOto","colab_type":"text"},"source":["# **Création des vectoriseurs pour chaque thème**\n","Va permettre d'évaluer la similarité entre une question utilisateur et toutes les questions/réponses de ce thème, de renvoyer ensuit la meilleure réponse."]},{"cell_type":"markdown","metadata":{"id":"JZobWsjRkuB_","colab_type":"text"},"source":["## **Importation**"]},{"cell_type":"code","metadata":{"id":"2D53B0gG6FLu","colab_type":"code","outputId":"545108a8-4076-4234-940d-ee682cee5126","executionInfo":{"status":"ok","timestamp":1580221768916,"user_tz":-60,"elapsed":933,"user":{"displayName":"Makk Claire","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBu1ZMQH3mpVpSuIuyc2aP_rat1JyDOTBBHgaS0uw=s64","userId":"01263964114703492317"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#!pip install -U spacy\n","#!python -m spacy download fr_core_news_md"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/My Drive/text_mining/Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3m-ybQDPjSaB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"f7067332-c125-46c2-c7e3-ec0446e3098b","executionInfo":{"status":"ok","timestamp":1580221875844,"user_tz":-60,"elapsed":28442,"user":{"displayName":"Makk Claire","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBu1ZMQH3mpVpSuIuyc2aP_rat1JyDOTBBHgaS0uw=s64","userId":"01263964114703492317"}}},"source":["import pandas as pd\n","import spacy\n","from collections import defaultdict, Counter\n","import numpy as np\n","from sklearn.externals import joblib\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","#Importation pré-traitement\n","import nltk\n","from nltk.tokenize.regexp import WordPunctTokenizer\n","from nltk.stem import SnowballStemmer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","import unicodedata\n","nlp = spacy.load('fr_core_news_md')\n","\n","#Strop words\n","def strip_accents(texte):\n","  return(unicodedata.normalize('NFKD', texte).encode('ASCII', 'ignore').decode('ASCII'))\n","nltk.download('stopwords')\n","sw=stopwords.words(\"french\")\n","sw += ['être','avoir','comment']\n","sw= [strip_accents(w) for w in sw]\n","\n","#Importation données\n","QA = pd.read_csv('../Data/Q&A.csv',sep=\";\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zz8zyTKxkyme","colab_type":"text"},"source":["## **Fonctions de pré-traitement**"]},{"cell_type":"code","metadata":{"id":"TE8EUh8_7OFM","colab_type":"code","colab":{}},"source":["#Définition des fonctions de prétraitement du texte\n","def lemmatise_text(text):\n","  tw_nlp = nlp(text)\n","  list_lem = [token.lemma_ for token in tw_nlp]\n","  text_lem = ' '.join(list_lem)\n","  return text_lem\n","\n","def stem_text(text):\n","  tokenizer = WordPunctTokenizer()\n","  stemmer = SnowballStemmer('french')\n","  liste_racines = [stemmer.stem(token) for token in tokenizer.tokenize(text)]\n","  return ' '.join(liste_racines)\n","\n","def normalise(text):\n","  #stop words, strip accent et lowercase vont être fait automatiquement\n","  text = text.replace('\\n','').replace('\\r','').split(\" \")\n","  text = \" \".join([i for i in text if i!=\"\"])\n","  lemmas = lemmatise_text(text) #lemme de notre texte\n","  stems = stem_text(lemmas) #stem de notre texte A VOIR\n","  return stems"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_3fmL_Hk-2U","colab_type":"text"},"source":["## **Vectoriseur pour chaque thème**"]},{"cell_type":"code","metadata":{"id":"3ss4IrpW8h3G","colab_type":"code","colab":{}},"source":["vectorizer_themes = {}\n","for t in QA.Themes.unique():\n","  vectorizer = TfidfVectorizer(lowercase=True, \n","                               stop_words=sw,\n","                               strip_accents='unicode',\n","                               norm='l2')\n","  QA_themes = QA[QA.Themes==t]\n","  ind = list(QA_themes.index)\n","  corpus_QA_themes = pd.Series(list(QA_themes.Answers) + list(QA_themes.Questions)).apply(normalise)\n","  dtm = vectorizer.fit_transform(corpus_QA_themes)\n","  vectorizer_themes[t]=[vectorizer,dtm,ind]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFbmREGr6JiI","colab_type":"code","outputId":"be4a6ec2-d148-479d-de97-c9b778cfc71d","executionInfo":{"status":"ok","timestamp":1580221909159,"user_tz":-60,"elapsed":521,"user":{"displayName":"Makk Claire","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBu1ZMQH3mpVpSuIuyc2aP_rat1JyDOTBBHgaS0uw=s64","userId":"01263964114703492317"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.externals import joblib\n","joblib.dump(vectorizer_themes, '../Data/vectorizer_themes.pkl')"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['../Data/vectorizer_themes.pkl']"]},"metadata":{"tags":[]},"execution_count":5}]}]}