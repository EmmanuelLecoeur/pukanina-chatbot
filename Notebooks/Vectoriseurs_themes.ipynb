{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaLUpDXwkOto"
   },
   "source": [
    "# **Création des vectoriseurs pour chaque thème**\n",
    "Va permettre d'évaluer la similarité entre une question utilisateur et toutes les questions/réponses de ce thème, de renvoyer ensuit la meilleure réponse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZobWsjRkuB_"
   },
   "source": [
    "## **Importation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1580221768916,
     "user": {
      "displayName": "Makk Claire",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBu1ZMQH3mpVpSuIuyc2aP_rat1JyDOTBBHgaS0uw=s64",
      "userId": "01263964114703492317"
     },
     "user_tz": -60
    },
    "id": "2D53B0gG6FLu",
    "outputId": "545108a8-4076-4234-940d-ee682cee5126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "/gdrive/My Drive/text_mining/Notebooks\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U spacy\n",
    "#!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28442,
     "status": "ok",
     "timestamp": 1580221875844,
     "user": {
      "displayName": "Makk Claire",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBu1ZMQH3mpVpSuIuyc2aP_rat1JyDOTBBHgaS0uw=s64",
      "userId": "01263964114703492317"
     },
     "user_tz": -60
    },
    "id": "3m-ybQDPjSaB",
    "outputId": "f7067332-c125-46c2-c7e3-ec0446e3098b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Importation pré-traitement\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import unicodedata\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "#Strop words\n",
    "def strip_accents(texte):\n",
    "  return(unicodedata.normalize('NFKD', texte).encode('ASCII', 'ignore').decode('ASCII'))\n",
    "nltk.download('stopwords')\n",
    "sw=stopwords.words(\"french\")\n",
    "sw += ['être','avoir','comment']\n",
    "sw= [strip_accents(w) for w in sw]\n",
    "\n",
    "#Importation données\n",
    "QA = pd.read_csv('../Data/Q_A.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz8zyTKxkyme"
   },
   "source": [
    "## **Fonctions de pré-traitement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TE8EUh8_7OFM"
   },
   "outputs": [],
   "source": [
    "#Définition des fonctions de prétraitement du texte\n",
    "def lemmatise_text(text):\n",
    "  tw_nlp = nlp(text)\n",
    "  list_lem = [token.lemma_ for token in tw_nlp]\n",
    "  text_lem = ' '.join(list_lem)\n",
    "  return text_lem\n",
    "\n",
    "def stem_text(text):\n",
    "  tokenizer = WordPunctTokenizer()\n",
    "  stemmer = SnowballStemmer('french')\n",
    "  liste_racines = [stemmer.stem(token) for token in tokenizer.tokenize(text)]\n",
    "  return ' '.join(liste_racines)\n",
    "\n",
    "def normalise(text):\n",
    "  #stop words, strip accent et lowercase vont être fait automatiquement\n",
    "  text = text.replace('\\n','').replace('\\r','').split(\" \")\n",
    "  text = \" \".join([i for i in text if i!=\"\"])\n",
    "  lemmas = lemmatise_text(text) #lemme de notre texte\n",
    "  stems = stem_text(lemmas) #stem de notre texte A VOIR\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_3fmL_Hk-2U"
   },
   "source": [
    "## **Vectoriseur pour chaque thème**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ss4IrpW8h3G"
   },
   "outputs": [],
   "source": [
    "vectorizer_themes = {}\n",
    "for t in QA.Themes.unique():\n",
    "  vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                               stop_words=sw,\n",
    "                               strip_accents='unicode',\n",
    "                               norm='l2')\n",
    "  QA_themes = QA[QA.Themes==t]\n",
    "  ind = list(QA_themes.index)\n",
    "  corpus_QA_themes = pd.Series(list(QA_themes.Answers) + list(QA_themes.Questions)).apply(normalise)\n",
    "  dtm = vectorizer.fit_transform(corpus_QA_themes)\n",
    "  vectorizer_themes[t]=[vectorizer,dtm,ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1580221909159,
     "user": {
      "displayName": "Makk Claire",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBu1ZMQH3mpVpSuIuyc2aP_rat1JyDOTBBHgaS0uw=s64",
      "userId": "01263964114703492317"
     },
     "user_tz": -60
    },
    "id": "wFbmREGr6JiI",
    "outputId": "be4a6ec2-d148-479d-de97-c9b778cfc71d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/vectorizer_themes.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(vectorizer_themes, '../Data/vectorizer_themes.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN8qV7VCt4KJeHno6B+GICQ",
   "collapsed_sections": [],
   "name": "Vectoriseurs_themes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
